{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCdNv3NWaRIW"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HOXbLwPxaRIZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SplINJkaRIZ"
      },
      "source": [
        "# Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "adtuGj3DaRIa",
        "outputId": "bd8bf724-e2dc-49df-8154-3fd0cd7af1f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6oAkffJ3aRIa"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "NUM_WORKERS = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r9_L3gqvaRIc"
      },
      "outputs": [],
      "source": [
        "ROOT_DIR = Path('.')\n",
        "DATA_DIR = ROOT_DIR / 'data'\n",
        "REPORTS_DIR = ROOT_DIR / 'reports'\n",
        "MODELS_DIR = REPORTS_DIR / 'models'\n",
        "RESULTS_DIR = REPORTS_DIR / 'results'\n",
        "RUNS_DIR = REPORTS_DIR / 'runs'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkSe5roFaRId"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fYnKNiFeaRId"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "     transforms.RandomHorizontalFlip(p=0.5),\n",
        "     transforms.RandomVerticalFlip(p=0.5)]\n",
        ")\n",
        "val_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "     transforms.RandomHorizontalFlip(p=0.5),\n",
        "     transforms.RandomVerticalFlip(p=0.5)]\n",
        ")\n",
        "test_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4EYWeP2SaRIe"
      },
      "outputs": [],
      "source": [
        "train_proportion = 0.9\n",
        "num_train = 50000\n",
        "\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(train_proportion * num_train))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_idx, val_idx = indices[:split], indices[split:]\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "val_sampler = SubsetRandomSampler(val_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2aAxDvk4aRIe",
        "outputId": "dfa73d15-fcfc-4912-d749-4a4774c63a03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:02<00:00, 84.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_dataset = datasets.CIFAR10(root=DATA_DIR, train=True,\n",
        "                                 download=True, transform=train_transform)\n",
        "\n",
        "val_dataset = datasets.CIFAR10(root=DATA_DIR, train=True,\n",
        "                               download=True, transform=val_transform)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root=DATA_DIR, train=False,\n",
        "                                download=True, transform=test_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-mvema9saRIf"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                          sampler=train_sampler, num_workers=NUM_WORKERS)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
        "                        sampler=val_sampler, num_workers=NUM_WORKERS)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                         num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q1jegINaRIf"
      },
      "source": [
        "# Aggregating functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zrryLKnoaRIg"
      },
      "outputs": [],
      "source": [
        "def arithmetic_mean(X, dim, keepdim):\n",
        "    return torch.mean(X, dim, keepdim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NWmJ9fZaaRIg"
      },
      "outputs": [],
      "source": [
        "def minimum(X, dim, keepdim):\n",
        "    return torch.min(X, dim, keepdim).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ui3cx4RLaRIh"
      },
      "outputs": [],
      "source": [
        "def product(X, dim, keepdim):\n",
        "    return torch.prod(X, dim=dim, keepdim=keepdim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hXxMpfPSaRIi"
      },
      "outputs": [],
      "source": [
        "def t_norm_lukasiewicz(X, dim, keepdim):\n",
        "    sum_X = torch.sum(X, dim=dim, keepdim=keepdim) - 1\n",
        "    return torch.max(sum_X, torch.tensor(0, device=X.device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JpekXW3MaRIi"
      },
      "outputs": [],
      "source": [
        "def t_norm_hamacker(X, dim, keepdim=False):\n",
        "    # Check if tensor has at least 2 elements along the specified dimension\n",
        "    if X.shape[dim] < 2:\n",
        "        raise ValueError(\"Tensor must have at least 2 elements along the specified dimension.\")\n",
        "\n",
        "    # Split the tensor into pairs of values along the specified dimension\n",
        "    tensor1 = X.narrow(dim, 0, 1)  # First element along the dimension\n",
        "    tensor2 = X.narrow(dim, 1, 1)  # Second element along the dimension\n",
        "\n",
        "    # Compute the Hamacher T-norm based on the formula\n",
        "    zero_mask = (tensor1 == 0) & (tensor2 == 0)\n",
        "    t_norm_result = torch.where(\n",
        "        zero_mask,\n",
        "        torch.zeros_like(tensor1),\n",
        "        (tensor1 * tensor2) / (tensor1 + tensor2 - tensor1 * tensor2)\n",
        "    )\n",
        "\n",
        "    # Reduce along the specified dimension\n",
        "    result = torch.prod(t_norm_result, dim=dim, keepdim=keepdim)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OqRYbKGLaRIj"
      },
      "outputs": [],
      "source": [
        "def maximum(X, dim, keepdim):\n",
        "    return torch.max(X, dim, keepdim).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "l0nT5RUOaRIj"
      },
      "outputs": [],
      "source": [
        "def t_conorm_lukasiewicz(X, dim, keepdim):\n",
        "    sum_X = torch.sum(X, dim=dim, keepdim=keepdim)\n",
        "    return torch.min(sum_X, torch.tensor(1.0, device=X.device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-oWGPmCPaRIj"
      },
      "outputs": [],
      "source": [
        "def t_conorm_hamacker(X, dim, keepdim):\n",
        "    product_X = torch.prod(X, dim=dim, keepdim=keepdim)\n",
        "\n",
        "    sum_X = torch.sum(X, dim=dim, keepdim=keepdim)\n",
        "\n",
        "    numerator = 2 * product_X - sum_X\n",
        "    denominator = product_X - 1\n",
        "\n",
        "    result = torch.where(\n",
        "        product_X == 1,\n",
        "        torch.tensor(1.0),\n",
        "        numerator / (denominator + 1e-10)  # Adding a small value for numerical stability.\n",
        "    )\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WV8eF-N_aRIj"
      },
      "outputs": [],
      "source": [
        "def u_min_max(X, dim, keepdim):\n",
        "    # Ensure we have at least two elements along the specified dimension\n",
        "    if X.shape[dim] < 2:\n",
        "        raise ValueError(\"Tensor must have at least 2 elements along the specified dimension.\")\n",
        "\n",
        "    # Shift the tensor to create pairs of adjacent elements along the specified dimension\n",
        "    tensor1 = X.narrow(dim, 0, X.shape[dim] - 1)  # Exclude the last element\n",
        "    tensor2 = X.narrow(dim, 1, X.shape[dim] - 1)  # Exclude the first element\n",
        "\n",
        "    # Condition to check if both values are in the range [0, 0.5]\n",
        "    condition = (tensor1 >= 0) & (tensor1 <= 0.5) & (tensor2 >= 0) & (tensor2 <= 0.5)\n",
        "\n",
        "    # Apply min where the condition is true, otherwise apply max\n",
        "    result = torch.where(condition, torch.min(tensor1, tensor2), torch.max(tensor1, tensor2))\n",
        "\n",
        "    # If keepdims is True, expand the reduced dimension to size 1\n",
        "    if keepdim:\n",
        "        result = result.unsqueeze(dim)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LQy1h1HlaRIj"
      },
      "outputs": [],
      "source": [
        "# ChatGPT version\n",
        "def u_min_max_chatgpt(X, dim, keepdim):\n",
        "   is_in_lower_range = (X <= 0.5).all(dim=dim, keepdim=keepdim)\n",
        "\n",
        "   min_X = torch.min(X, dim=dim, keepdim=keepdim).values\n",
        "   max_X = torch.max(X, dim=dim, keepdim=keepdim).values\n",
        "\n",
        "   return torch.where(is_in_lower_range, min_X, max_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "a4vs69DbaRIj"
      },
      "outputs": [],
      "source": [
        "def u_l_l(X, dim, keepdim):\n",
        "    # Ensure we have at least two elements along the specified dimension\n",
        "    if X.shape[dim] < 2:\n",
        "        raise ValueError(\"Tensor must have at least 2 elements along the specified dimension.\")\n",
        "\n",
        "    # Shift the tensor to create pairs of adjacent elements along the specified dimension\n",
        "    tensor1 = X.narrow(dim, 0, X.shape[dim] - 1)  # Exclude the last element\n",
        "    tensor2 = X.narrow(dim, 1, X.shape[dim] - 1)  # Exclude the first element\n",
        "\n",
        "    # Conditions\n",
        "    condition1 = (tensor1 >= 0) & (tensor1 <= 0.5) & (tensor2 >= 0) & (tensor2 <= 0.5)\n",
        "    condition2 = (tensor1 >= 0.5) & (tensor1 <= 1) & (tensor2 >= 0.5) & (tensor2 <= 1)\n",
        "\n",
        "    # Calculate each case\n",
        "    result1 = torch.max(tensor1 + tensor2 - 1, torch.zeros_like(tensor1))  # max(x + y - 1, 0)\n",
        "    result2 = torch.min(tensor1 + tensor2, torch.ones_like(tensor1))       # min(x + y, 1)\n",
        "    result3 = torch.max(tensor1, tensor2)                                  # max(x, y)\n",
        "\n",
        "    # Apply conditions\n",
        "    result = torch.where(condition1, result1, torch.where(condition2, result2, result3))\n",
        "\n",
        "    # Handle keepdims option\n",
        "    if keepdim:\n",
        "        result = result.unsqueeze(dim)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-qvp15RTaRIk"
      },
      "outputs": [],
      "source": [
        "class ChoquetLayer(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(ChoquetLayer, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.w = None\n",
        "\n",
        "    def forward(self, x, dim, keepdim):\n",
        "        input_size = x.size(-1)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        if self.w is None or self.w.size(0) != batch_size:\n",
        "            self.w = nn.Parameter(torch.randn(input_size))\n",
        "\n",
        "        x_sorted, indices = torch.sort(x, descending=True, dim=-1)  # (batch_size, input_size)\n",
        "        v_Ai = self.compute_v_Ai(indices)  # (batch_size, n+1)\n",
        "        v_delta = v_Ai[..., :-1] - v_Ai[..., 1:]  # (batch_size, input_size)\n",
        "        x = torch.sum(x_sorted * v_delta, dim=-1)  # (batch_size)\n",
        "\n",
        "        if keepdim:\n",
        "          x.unsqueeze(dim)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def compute_v_Ai(self, indices):\n",
        "        w_sorted = self.w[indices]\n",
        "        v_Ai = torch.cumsum(w_sorted, dim=-1)\n",
        "        zeros = torch.zeros(*v_Ai.shape[:-1], 1).to(v_Ai.device)\n",
        "        v_Ai = torch.cat((zeros, v_Ai), dim=-1)\n",
        "        return v_Ai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lj-tSUZoaRIk"
      },
      "source": [
        "# Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_1qzWROpaRIk"
      },
      "outputs": [],
      "source": [
        "class AggPoolingLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, function, kernel_size, stride, padding= [0,0,0,0], dim = -1, keepdim = False):\n",
        "        super().__init__()\n",
        "\n",
        "        # Una tupla de 2 elementos con los tamaÃ±os [ð‘˜1,ð‘˜2] de cada ventana a tratar\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        # Tupla de 2 elementos que indican el nÃºmero de elementos (en filas y columnas) que\n",
        "        # deben saltarse tras reducir cada ventana, hasta encontrar la siguiente a tratar.\n",
        "        self.stride = stride\n",
        "\n",
        "        # Tupla de 4 elementos de la forma [ð‘ð‘Žð‘‘_ð‘™ð‘’ð‘“ð‘¡,ð‘ð‘Žð‘‘_ð‘Ÿð‘–ð‘”â„Žð‘¡,ð‘ð‘Žð‘‘_ð‘¢ð‘,ð‘ð‘Žð‘‘_ð‘‘ð‘œð‘¤ð‘›] que indica el\n",
        "        # nÃºmero de nuevas filas o columnas a aÃ±adir a la entrada, previo a aplicar la agregaciÃ³n.\n",
        "        self.padding = padding\n",
        "\n",
        "        # Define function and characteristics\n",
        "        self.function = function\n",
        "        self.dim = dim\n",
        "        self.keepdim = keepdim\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        # Normalize\n",
        "        maximum = torch.max(X)\n",
        "        minimum = torch.min(X)\n",
        "        X = (X-minimum)/(maximum-minimum)\n",
        "\n",
        "        # AÃ±adir columnas/filas segÃºn padding\n",
        "        X_pad = F.pad(X, pad=self.padding, mode='constant', value=0)\n",
        "\n",
        "        # Vamos extrayendo las ventanas a agregar y colocÃ¡ndolas en filas\n",
        "        X_aux = X_pad.unfold(2, size=self.kernel_size[0], step=self.stride[0]).unfold(3, size=self.kernel_size[1], step=self.stride[1])\n",
        "\n",
        "        # Ponemos el formato correcto\n",
        "        X_aux = X_aux.reshape([X_aux.shape[0], X_aux.shape[1], X_aux.shape[2], X_aux.shape[3], X_aux.shape[4] * X_aux.shape[5]])\n",
        "\n",
        "        # Agg Func\n",
        "        Y_temp = self.function(X_aux, dim = self.dim, keepdim = self.keepdim)\n",
        "\n",
        "        # Denormalize\n",
        "        Y = minimum + (maximum-minimum) * Y_temp\n",
        "\n",
        "        return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "gkaPYqtVaRIl"
      },
      "outputs": [],
      "source": [
        "class OWAPoolingLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, kernel_size, stride, padding= [0,0,0,0], dim = -1, keepdim = False):\n",
        "        super().__init__()\n",
        "\n",
        "        # Una tupla de 2 elementos con los tamaÃ±os [ð‘˜1,ð‘˜2] de cada ventana a tratar\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        # Tupla de 2 elementos que indican el nÃºmero de elementos (en filas y columnas) que\n",
        "        # deben saltarse tras reducir cada ventana, hasta encontrar la siguiente a tratar.\n",
        "        self.stride = stride\n",
        "\n",
        "        # Tupla de 4 elementos de la forma [ð‘ð‘Žð‘‘_ð‘™ð‘’ð‘“ð‘¡,ð‘ð‘Žð‘‘_ð‘Ÿð‘–ð‘”â„Žð‘¡,ð‘ð‘Žð‘‘_ð‘¢ð‘,ð‘ð‘Žð‘‘_ð‘‘ð‘œð‘¤ð‘›] que indica el\n",
        "        # nÃºmero de nuevas filas o columnas a aÃ±adir a la entrada, previo a aplicar la agregaciÃ³n.\n",
        "        self.padding = padding\n",
        "\n",
        "        # Define characteristics\n",
        "        self.dim = dim\n",
        "        self.keepdim = keepdim\n",
        "\n",
        "        # Weights\n",
        "        self.weight = nn.Parameter(torch.ones(1, self.kernel_size[0] * self.kernel_size[1]))\n",
        "\n",
        "\n",
        "    def funcionOWA(self, X):\n",
        "\n",
        "        tensor_ordered = torch.sort(X, descending = True)\n",
        "        weight_norm = torch.nn.functional.softmax(self.weight, dim = self.dim)\n",
        "        output = torch.sum(tensor_ordered[0] * weight_norm, dim = self.dim, keepdim = self.keepdim)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        # Normalize\n",
        "        maximum = torch.max(X)\n",
        "        minimum = torch.min(X)\n",
        "        X = (X-minimum)/(maximum-minimum)\n",
        "\n",
        "        # AÃ±adir columnas/filas segÃºn padding\n",
        "        X_pad = F.pad(X, pad=self.padding, mode='constant', value=0)\n",
        "\n",
        "        # Vamos extrayendo las ventanas a agregar y colocÃ¡ndolas en filas\n",
        "        X_aux = X_pad.unfold(2, size=self.kernel_size[0], step=self.stride[0]).unfold(3, size=self.kernel_size[1], step=self.stride[1])\n",
        "\n",
        "        # Ponemos el formato correcto\n",
        "        X_aux = X_aux.reshape([X_aux.shape[0], X_aux.shape[1], X_aux.shape[2], X_aux.shape[3], X_aux.shape[4] * X_aux.shape[5]])\n",
        "\n",
        "        # Agg Func\n",
        "        Y_temp = self.funcionOWA(X_aux)\n",
        "\n",
        "        # Denormalize\n",
        "        Y = minimum + (maximum-minimum) * Y_temp\n",
        "\n",
        "        return Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHAyDZeoaRIl"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "qelKXZzRaRIm"
      },
      "outputs": [],
      "source": [
        "class LeNetModel(nn.Module):\n",
        "\n",
        "    def __init__(self, function, conv_filters=[64, 64], linear_sizes=[384, 192], num_classes=10):\n",
        "        super(LeNetModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, conv_filters[0], [2,2], [1,1])\n",
        "        self.pool1 = AggPoolingLayer(function, [2,2], [2,2])\n",
        "        self.conv2 = nn.Conv2d(conv_filters[0], conv_filters[1], [2,2], [1,1])\n",
        "        self.pool2 = AggPoolingLayer(function, [2,2], [2,2])\n",
        "        self.fc1 = nn.Linear(conv_filters[1]*7*7, linear_sizes[0])\n",
        "        self.fc2 = nn.Linear(linear_sizes[0], linear_sizes[1])\n",
        "        self.fc3 = nn.Linear(linear_sizes[1], num_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng8cvSRRaRIn"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZHy7wYvxaRIn"
      },
      "outputs": [],
      "source": [
        "writer = SummaryWriter(log_dir=RUNS_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "YLkCtKCxaRIn"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, val_loader=None, num_epochs=20, device=device):\n",
        "    train_acc = []\n",
        "    train_loss = []\n",
        "    # if val_loader is not None:\n",
        "    val_acc = []\n",
        "    val_loss = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        count_evaluated = 0\n",
        "        count_correct = 0\n",
        "\n",
        "        for batch_idx, data in enumerate(train_loader, 0):\n",
        "            model.train()\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            count_evaluated += inputs.shape[0]\n",
        "            count_correct += torch.sum(labels == torch.max(outputs, dim=1)[1])\n",
        "\n",
        "        print('Training: [%d, %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / (batch_idx+1)))\n",
        "\n",
        "        train_loss.append(running_loss / (batch_idx+1))\n",
        "        train_acc.append(float(count_correct) / count_evaluated)\n",
        "\n",
        "        if val_loader is not None:\n",
        "            running_loss_val = 0.0\n",
        "            count_evaluated = 0\n",
        "            count_correct = 0\n",
        "            model.eval()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for val_batch_idx, data_val in enumerate(val_loader, 0):\n",
        "                    inputs_val, labels_val = data_val[0].to(device), data_val[1].to(device)\n",
        "                    outputs_val = model(inputs_val)\n",
        "                    loss = criterion(outputs_val, labels_val)\n",
        "                    running_loss_val += loss.item()\n",
        "                    count_evaluated += inputs_val.shape[0]\n",
        "                    count_correct += torch.sum(labels_val == torch.max(outputs_val, dim=1)[1])\n",
        "\n",
        "                val_loss.append(running_loss_val / (val_batch_idx + 1))\n",
        "                acc_val = float(count_correct) / count_evaluated\n",
        "\n",
        "                print('Validation: epoch %d - acc: %.3f' %\n",
        "                            (epoch + 1, acc_val))\n",
        "                val_acc.append(acc_val)\n",
        "\n",
        "        # Tensorboard\n",
        "        writer.add_scalar('Loss/Validation', val_loss[-1], global_step=epoch)\n",
        "        writer.add_scalar('Accuracy/Validation', val_acc[-1], global_step=epoch)\n",
        "        writer.add_scalar('Loss/Train', train_loss[-1], global_step=epoch)\n",
        "        writer.add_scalar('Accuracy/Train', train_acc[-1], global_step=epoch)\n",
        "        for name, param in model.named_parameters():\n",
        "            writer.add_histogram(f\"Parameters/{name}\", param, epoch)\n",
        "            if param.grad is not None:\n",
        "                writer.add_histogram(f\"Gradients/{name}\", param.grad, epoch)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky_kj4JWaRIo"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "5AZ1mU9XaRIo"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader, criterion, device='cuda'):\n",
        "    with torch.no_grad():\n",
        "        number_samples = 0\n",
        "        number_correct = 0\n",
        "        running_loss_test = 0.0\n",
        "        for test_batch_idx, data_test in enumerate(test_loader, 0):\n",
        "            inputs_test, labels_test = data_test[0].to(device), data_test[1].long().to(device)\n",
        "            outputs_test = model(inputs_test)\n",
        "            loss = criterion(outputs_test, labels_test)\n",
        "            running_loss_test += loss.cpu().numpy()\n",
        "\n",
        "            _, outputs_class = torch.max(outputs_test, dim=1)\n",
        "            number_correct += torch.sum(outputs_class == labels_test).cpu().numpy()\n",
        "            number_samples += len(labels_test)\n",
        "\n",
        "        acc_test = number_correct / number_samples\n",
        "\n",
        "        print('Test - Accuracy: %.3f' % acc_test)\n",
        "        print('Test - CrossEntropy: %.3f' % (running_loss_test / (test_batch_idx+1)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = LeNetModel(arithmetic_mean)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "trained_model = train(model, train_loader, criterion, optimizer, val_loader=val_loader)"
      ],
      "metadata": {
        "id": "PhCwjeFcam4k",
        "outputId": "43d76831-4587-4960-dcab-9294ae744989",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [1,   704] loss: 1.568\n",
            "Validation: epoch 1 - acc: 0.511\n",
            "Training: [2,   704] loss: 1.231\n",
            "Validation: epoch 2 - acc: 0.585\n",
            "Training: [3,   704] loss: 1.085\n",
            "Validation: epoch 3 - acc: 0.618\n",
            "Training: [4,   704] loss: 0.984\n",
            "Validation: epoch 4 - acc: 0.654\n",
            "Training: [5,   704] loss: 0.904\n",
            "Validation: epoch 5 - acc: 0.662\n",
            "Training: [6,   704] loss: 0.843\n",
            "Validation: epoch 6 - acc: 0.675\n",
            "Training: [7,   704] loss: 0.791\n",
            "Validation: epoch 7 - acc: 0.686\n",
            "Training: [8,   704] loss: 0.744\n",
            "Validation: epoch 8 - acc: 0.698\n",
            "Training: [9,   704] loss: 0.700\n",
            "Validation: epoch 9 - acc: 0.707\n",
            "Training: [10,   704] loss: 0.656\n",
            "Validation: epoch 10 - acc: 0.699\n",
            "Training: [11,   704] loss: 0.621\n",
            "Validation: epoch 11 - acc: 0.722\n",
            "Training: [12,   704] loss: 0.587\n",
            "Validation: epoch 12 - acc: 0.712\n",
            "Training: [13,   704] loss: 0.558\n",
            "Validation: epoch 13 - acc: 0.718\n",
            "Training: [14,   704] loss: 0.531\n",
            "Validation: epoch 14 - acc: 0.724\n",
            "Training: [15,   704] loss: 0.504\n",
            "Validation: epoch 15 - acc: 0.734\n",
            "Training: [16,   704] loss: 0.473\n",
            "Validation: epoch 16 - acc: 0.717\n",
            "Training: [17,   704] loss: 0.448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iiQKSuMdcozt"
      },
      "execution_count": 71,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}